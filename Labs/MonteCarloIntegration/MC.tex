\lab{Monte-Carlo Integration}{Monte-Carlo Integration}
\objective{Use Monte-Carlo integration to estimate areas.}

Some multivariable integrals which are critical in applications are impossible to evaulate symbolically.
For example, the integral of the joint normal distribution
\[
\int_{\Omega} \frac{1}{\sqrt{(2 \pi)^k}} e^{- \frac{\bold{x}^T\bold{x}}{2}}
\]
is ubiquitous in statistics.
However, the integrand does not have a symbolic antiderivative.
This means we must use numerical methods to evaluate this integral.

The standard technique for numerically evaluating multivariable integrals is \emph{Monte-Carlo Integration}.
Monte-Carlo (MC) integration is radically different from 1-dimensional techniques like Simpson's rule, relying on probability to calculate the integral.
Although it converges slowly, MC integration is frequently used to evaluate multivariable integrals because the higher-dimensional analogs of methods like Simpson's rule are inefficient. 

\section*{A motivating example}
Suppose we want to numerically compute the area of a circle of radius 1.
From analytic methods, we know the answer is $\pi$.
Empirically, we can estimate this quantity by randomly choosing points in a $2 \times 2$ square.
The percent of points that land in the inscribed circle, times the area of the square, should approximately equal the area of the circle (see Figure \ref{fig:MCCircle}).

\begin{figure}
\includegraphics[width=.7\textwidth]{MC_Circle.pdf}
\caption{Finding the area of a circle using random points}
\label{fig:MCCircle}
\end{figure}

We do this in NumPy as follows. First generate 500 random points in the square $[0,1] \times [0,1]$.
\begin{lstlisting}
>>> numPoints = 500
>>> points = np.random.rand(2, numPoints)
\end{lstlisting}
We rescale and shift these points to be uniformly distributed in $[-1,1]\times[-1,1]$.
\begin{lstlisting}
>>> points = points*2-1
\end{lstlisting}
Next we compute the number of points in the unit circle.
The function \li{np.hypot(a, b)} returns the norm of the vector $(a, b)$.
\begin{lstlisting}
>>> # Create a mask of points in the circle
>>> circleMask = np.hypot(points[0,:], points[1,:]) <= 1
>>> # Count how many there are
>>> numInCircle = np.count_nonzero(circleMask)
\end{lstlisting}
Finally, we approximate the area.
\begin{lstlisting}
>>> # Area is approximately (area of the square)*(num points in circle)/(total num points)
>>> 4.*numInCircle/numPoints
3.024
\end{lstlisting}
This differs from $\pi$ by about 0.117.






We analyze the error of the MC method by repeating this experiment for many values of \li{numPoints} and plotting the errors.
The result is the blue line in Figure \ref{fig:mc_error}.
The error appears to be proportional to $1/\sqrt{N}$ where $N=$\li{numPoints} (the green line in Figure \ref{fig:mc_error}).
This means that to divide the error by10, we must sample \emph{100 times} more points.

\begin{figure}
\includegraphics[width=.7\textwidth]{mc_error.pdf}
\caption{The Monte-Carlo integration method was used to compute the area of a circle of radius 1. 
The blue line plots the average error in 100 runs of the MC method on $N$ sample points, where $N$ appears on the horizontal axis.
The green line is a plot of $1/\sqrt{N}$. }
\label{fig:mc_error}
\end{figure}

This is a slow convergence rate, but it is independent of the number of dimensions of the problem. 
This dimension independence is what makes the MC method useful for multivariable integrals.

\section*{Monte-Carlo Integration}

You can calculate the area of the unit circle with the following integration problem:
\[
\mbox{Area of unit circle } = \int_{[-1,1]\times[-1,1]} f(x,y) dA
\]
where
\begin{equation}\label{equ:mc_func}
f(x,y) = \begin{cases} 1 &\mbox{ if $x$,$y$ is in the unit circle} \\ 0 &\mbox{ otherwise.} \end{cases}
\end{equation}

We can use a random-points method as above to approximate any integral.
Suppose we with to evaluate
\[
\int_\Omega f(x) dV.
\]
We can approximate this integral using the formula
\begin{equation}\label{equ:mc_eq}
\int_\Omega f(x) dV \approx V(\Omega) \frac{1}{N} \sum_{i=1}^N f(x_i),
\end{equation}
where $x_i$ are uniformly distributed random vectors in $\Omega$ and $V(\Omega)$ is the volume of $\Omega$.
This is the formula for Monte-Carlo Integration. 


In our example, $\Omega$ was the box $[-1,1] \times [-1,1]$ and $f$ was the function defined in \eqref{equ:mc_func}.
Then $\sum_{i=1}^N f(x_i)$ is the number of points in the unit circle, $N$ is the total number of points, and \eqref{equ:mc_eq} is the same as the formula we derived previously.

The intuition behind \eqref{equ:mc_eq} is that $\frac{1}{N} \sum_{i=1}^N f(x_i)$ approximates the average value of $f$ on $\Omega$.
We multiply this (approximate) average value by the volume of $\Omega$ to get the (approximate) integral of $f$ on $\Omega$.

As an 1-dimensional example consider the integral 
\[
\int_0^1 x dx \approx (1-0)\frac{1}{N} \sum_{i=1}^N x_i=\frac{1}{N} \sum_{i=1}^N x_i.
\]

The integral on the left-hand-side is $1/2$. 
In the approximation on the right-hand-side, $x_i$ is drawn from a uniform distribution on $[0,1]$. 
The average of $N$ such draws will converge to $1/2$. 

\begin{problem}
\label{prob:mc}
Implement Monte-Carlo integration with the following function.
Your implementation should run the Monte-Carlo algorithm several times and return the average of those runs.
\begin{lstlisting}
def mc_int(f, mins, maxs, numPoints=500, numIters=100):
    '''
    Use Monte-Carlo integration to approximate the integral of f
    on the box defined by mins and maxs.
    
    INPUTS:
    f         - A function handle. Should accept a 1-D NumPy array 
    	 	as input.
    mins      - A 1-D NumPy array of the minimum bounds on integration.
    maxs      - A 1-D NumPy array of the maximum bounds on integration.
    numPoints - An integer specifying the number of points to sample in 
    		the Monte-Carlo method. Defaults to 500.
    numIters - An integer specifying the number of times to run the 
    		Monte Carlo algorithm. Defaults to 100.
		
    ALGORITHM:
    Run the Monte-Carlo algorithm `numIters' times and return the average
    of these runs.
                
    EXAMPLES:
    >>> f = lambda x: np.hypot(x[0], x[1]) <= 1
    >>> # Integral over the square [-1,1] x [-1,1] should be pi
    >>> mc_int(f, np.array([-1,-1]), np.array([1,1]))
    3.1290400000000007
    '''
\end{lstlisting}

Hints:
\begin{enumerate}
\item To create a random array of points on which to evaluate \li{f}, first create a random array of points in $[0,1] \times \ldots \times [0,1]$.
Then multiply this array by the appropriate vector to stretch it the right amount in each direction.
Finally, add the appropriate vector to shift the points to the right location.
\item You can evaluate \li{f} on an array of points in one line using \li{np.apply_along_axis()}.
\end{enumerate}
\end{problem}

\begin{problem}
We have already claimed that Monte Carlo integration is more effective than other numerical integration techniques in higher dimensions. We use the SciPy function \li{scipy.integrate.nquad} to demonstrate this. Compare the performance of \li{scipy.integrate.nquad} to your \li{mc_int} function using $f(x)=\prod_{i=1}^{n} 2x_i$ on $[0,1]\times \dotsb \times [0,1]$, for $n=1,2,3$ and 4. Set \li{numPoints=50}. We demonstrate below how to use the \li{nquad} function.

\begin{lstlisting}
# nquad expects a function with n arguments. We must therefore use a * to
# unpack the array x, and then convert it back to an array inside of f.
def f(*x):
    return np.prod(2*np.array(x))
\end{lstlisting}

We are now ready to call \li{mc_int} and \li{nquad} on \li{f}.

\begin{lstlisting}
>>> from scipy.integrate import nquad
>>> n = 2
>>> mins = np.zeros(n)
>>> maxs = np.ones(n)
>>> ranges = [mins[i], maxs[i] for i in xrange(n)]

>>> %timeit nquad(f, ranges)
100 loops, best of 3: 13.9 ms per loop
>>> %timeit mc_int(f, mins, maxs, numPoints=50)
1 loops, best of 3: 296 ms per loop

\end{lstlisting}
\end{problem}

\begin{comment}
\begin{problem}
\label{prob:mc_test}
The exact value of the integral of
\[
f(w,x,y,z) = sin(x) y^5 -y^3 + zw + yz^3
\]
on $[-1,1]x[-1,1]x[-1,1]x[-1,1]$ is 0.
\begin{enumerate}
\item Run the function \li{mc_int()} you wrote in Problem \ref{prob:mc} on $f$ 10 sample points.
Do this 100 times and take the average. What is the error?
\item Repeat part (1) of this problem with 100, 1000, and 10000 sample points. 
Plot the errors of your estimates.
\end{enumerate}
\end{problem}
\end{comment}

One application of Monte Carlo integration is integrating probability density functions that do not have closed form solutions.

\begin{comment}
\begin{problem}
The standard normal distribution is an important object of study in probability and statistic.
It is defined by the density function $\frac{1}{\sqrt{2 \pi}} e^{- \frac{x^2}{2}}$.
(Here we are assuming a mean of $0$ and a variance of $1$).
This is a function that cannot be integrated symbolically.
We can use monte carlo integration to estimate the probability that a normally distributed random variable will take a value below a given point.
The probability that the random variable we are considering is less than (or equal to) a given value $x$ is
\[\int_{-\infty}^x \frac{1}{\sqrt{2 \pi}} e^{- \frac{t^2}{2}} dt\]
This function is essentially zero for values of $x$ that lie reasonably far from the mean, so we can estimate this probability by integrating from $-5$ to $x$ instead.

Compare your result at $x = 1$ with the output of the code
\begin{lstlisting}
from scipy.stats import norm
N = norm()
N.cdf(1)
\end{lstlisting}
\end{problem}
\end{comment}

\begin{problem}
The joint normal distribution of $N$ independent random variables with mean 0 and variance 1 is
\[
f(\x) = \frac{1}{\sqrt{(2 \pi)^N}} e^{- \frac{\x^T\x}{2}}.
\]
The integral of $f(\x)$ over a box is the probability that a draw from the distribution will be in the box.
However, $f(\x)$ does not have a symbolic antiderivative.
\begin{enumerate}
\item The integral of this function on $B = [-1,1]\times [-1,1]\times[-1,1] \subset \mathbb{R}^3$ can be computed in SciPy with the following code.
\begin{lstlisting}
import scipy.stats as stats

# Define the bounds of the box to integrate over
mins = np.array([-1, -1, -1])
maxs = np.array([1, 1, 1])

# Each variable has mean 0 and variance 1
means = np.zeros(3)
covs = np.ones(3)

# Compute the integral
value, inform = stats.mvn.mvnun(mins, maxs, means, covs)
\end{lstlisting}
Then \li{value} is the integral of $f(\x)$ on $B$.
Use SciPy to integrate $f(\x)$ on $\Omega=[-0.5, 0.75]\times[0,1]\times[0, 0.5]\times[0,1] \subset \mathbb{R}^4$.

\item Use the function \li{mc_int()} you wrote in Problem \ref{prob:mc} to integrate $f(\x)$ on $\Omega$ with 10, 100, 1000, and 10,000 sample points. 
Plot the errors of your estimates.
\end{enumerate}
\end{problem}


\section*{A caution}
You can run into trouble if you try to use MC integration on an integral that does not converge.
For example, we may attempt to evaluate
\[
\int_0^1 \frac{1}{x}
\]
with MC integragtion using the following code.
\begin{lstlisting}
>>> k = 5000
>>> np.mean(1/np.random.rand(k,1))
21.237332864358656
\end{lstlisting}

Since this code returns a finite value, so we could assume that this integral has a finite value.
In fact, the integral is infinite.
We could discover this emprically by using larger and larger values of $k$, and noting that MC integration returns larger and larger values.


\begin{comment}
\begin{problem}
\label{prob:mc_flawed}
Create a new function (based upon the function from Problem \ref{prob:mc}) that uses a ``flawed'' random number generator that doesn't produce numbers between $-.95$ and $-1$. Test your method on the function from Problem \ref{prob:mc_test}. How bad is the error? 
\end{problem}
\end{comment}
